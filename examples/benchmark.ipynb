{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import traceback\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import EulerAncestralDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NVIDIA L4', '11.8', 8700)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.cuda.current_device()), torch.version.cuda, torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._C._nn.scaled_dot_product_attention>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",    \n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "# pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StableDiffusionPipeline in module diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion object:\n",
      "\n",
      "class StableDiffusionPipeline(diffusers.pipelines.pipeline_utils.DiffusionPipeline, diffusers.loaders.TextualInversionLoaderMixin, diffusers.loaders.LoraLoaderMixin, diffusers.loaders.FromCkptMixin)\n",
      " |  StableDiffusionPipeline(vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers, safety_checker: diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker, feature_extractor: transformers.models.clip.image_processing_clip.CLIPImageProcessor, requires_safety_checker: bool = True)\n",
      " |  \n",
      " |  Pipeline for text-to-image generation using Stable Diffusion.\n",
      " |  \n",
      " |  This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
      " |  library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
      " |  \n",
      " |  In addition the pipeline inherits the following loading methods:\n",
      " |      - *Textual-Inversion*: [`loaders.TextualInversionLoaderMixin.load_textual_inversion`]\n",
      " |      - *LoRA*: [`loaders.LoraLoaderMixin.load_lora_weights`]\n",
      " |      - *Ckpt*: [`loaders.FromCkptMixin.from_ckpt`]\n",
      " |  \n",
      " |  as well as the following saving methods:\n",
      " |      - *LoRA*: [`loaders.LoraLoaderMixin.save_lora_weights`]\n",
      " |  \n",
      " |  Args:\n",
      " |      vae ([`AutoencoderKL`]):\n",
      " |          Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
      " |      text_encoder ([`CLIPTextModel`]):\n",
      " |          Frozen text-encoder. Stable Diffusion uses the text portion of\n",
      " |          [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
      " |          the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
      " |      tokenizer (`CLIPTokenizer`):\n",
      " |          Tokenizer of class\n",
      " |          [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
      " |      unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n",
      " |      scheduler ([`SchedulerMixin`]):\n",
      " |          A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
      " |          [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
      " |      safety_checker ([`StableDiffusionSafetyChecker`]):\n",
      " |          Classification module that estimates whether generated images could be considered offensive or harmful.\n",
      " |          Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n",
      " |      feature_extractor ([`CLIPImageProcessor`]):\n",
      " |          Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StableDiffusionPipeline\n",
      " |      diffusers.pipelines.pipeline_utils.DiffusionPipeline\n",
      " |      diffusers.configuration_utils.ConfigMixin\n",
      " |      diffusers.loaders.TextualInversionLoaderMixin\n",
      " |      diffusers.loaders.LoraLoaderMixin\n",
      " |      diffusers.loaders.FromCkptMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: Union[str, List[str]] = None, height: Optional[int] = None, width: Optional[int] = None, num_inference_steps: int = 50, guidance_scale: float = 7.5, negative_prompt: Union[str, List[str], NoneType] = None, num_images_per_prompt: Optional[int] = 1, eta: float = 0.0, generator: Union[torch._C.Generator, List[torch._C.Generator], NoneType] = None, latents: Optional[torch.FloatTensor] = None, prompt_embeds: Optional[torch.FloatTensor] = None, negative_prompt_embeds: Optional[torch.FloatTensor] = None, output_type: Optional[str] = 'pil', return_dict: bool = True, callback: Optional[Callable[[int, int, torch.FloatTensor], NoneType]] = None, callback_steps: int = 1, cross_attention_kwargs: Optional[Dict[str, Any]] = None, guidance_rescale: float = 0.0)\n",
      " |          Function invoked when calling the pipeline for generation.\n",
      " |      \n",
      " |          Args:\n",
      " |              prompt (`str` or `List[str]`, *optional*):\n",
      " |                  The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
      " |                  instead.\n",
      " |              height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
      " |                  The height in pixels of the generated image.\n",
      " |              width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
      " |                  The width in pixels of the generated image.\n",
      " |              num_inference_steps (`int`, *optional*, defaults to 50):\n",
      " |                  The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
      " |                  expense of slower inference.\n",
      " |              guidance_scale (`float`, *optional*, defaults to 7.5):\n",
      " |                  Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
      " |                  `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
      " |                  Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
      " |                  1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
      " |                  usually at the expense of lower image quality.\n",
      " |              negative_prompt (`str` or `List[str]`, *optional*):\n",
      " |                  The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
      " |                  `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
      " |                  less than `1`).\n",
      " |              num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
      " |                  The number of images to generate per prompt.\n",
      " |              eta (`float`, *optional*, defaults to 0.0):\n",
      " |                  Corresponds to parameter eta (Î·) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
      " |                  [`schedulers.DDIMScheduler`], will be ignored for others.\n",
      " |              generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
      " |                  One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
      " |                  to make generation deterministic.\n",
      " |              latents (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
      " |                  generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
      " |                  tensor will ge generated by sampling using the supplied random `generator`.\n",
      " |              prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
      " |                  provided, text embeddings will be generated from `prompt` input argument.\n",
      " |              negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
      " |                  weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
      " |                  argument.\n",
      " |              output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
      " |                  The output format of the generate image. Choose between\n",
      " |                  [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
      " |              return_dict (`bool`, *optional*, defaults to `True`):\n",
      " |                  Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
      " |                  plain tuple.\n",
      " |              callback (`Callable`, *optional*):\n",
      " |                  A function that will be called every `callback_steps` steps during inference. The function will be\n",
      " |                  called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
      " |              callback_steps (`int`, *optional*, defaults to 1):\n",
      " |                  The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
      " |                  called at every step.\n",
      " |              cross_attention_kwargs (`dict`, *optional*):\n",
      " |                  A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
      " |                  `self.processor` in\n",
      " |                  [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
      " |              guidance_rescale (`float`, *optional*, defaults to 0.7):\n",
      " |                  Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are\n",
      " |                  Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `Ï†` in equation 16. of\n",
      " |                  [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).\n",
      " |                  Guidance rescale factor should fix overexposure when using zero terminal SNR.\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |          ```py\n",
      " |          >>> import torch\n",
      " |          >>> from diffusers import StableDiffusionPipeline\n",
      " |      \n",
      " |          >>> pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
      " |          >>> pipe = pipe.to(\"cuda\")\n",
      " |      \n",
      " |          >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n",
      " |          >>> image = pipe(prompt).images[0]\n",
      " |          ```\n",
      " |      \n",
      " |      \n",
      " |          Returns:\n",
      " |              [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
      " |              [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
      " |              When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
      " |              list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
      " |              (nsfw) content, according to the `safety_checker`.\n",
      " |  \n",
      " |  __init__(self, vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers, safety_checker: diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker, feature_extractor: transformers.models.clip.image_processing_clip.CLIPImageProcessor, requires_safety_checker: bool = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_inputs(self, prompt, height, width, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None)\n",
      " |  \n",
      " |  decode_latents(self, latents)\n",
      " |  \n",
      " |  disable_vae_slicing(self)\n",
      " |      Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n",
      " |      computing decoding in one step.\n",
      " |  \n",
      " |  disable_vae_tiling(self)\n",
      " |      Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this method will go back to\n",
      " |      computing decoding in one step.\n",
      " |  \n",
      " |  enable_model_cpu_offload(self, gpu_id=0)\n",
      " |      Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n",
      " |      to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n",
      " |      method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n",
      " |      `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n",
      " |  \n",
      " |  enable_sequential_cpu_offload(self, gpu_id=0)\n",
      " |      Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n",
      " |      text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n",
      " |      `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n",
      " |      Note that offloading happens on a submodule basis. Memory savings are higher than with\n",
      " |      `enable_model_cpu_offload`, but performance is lower.\n",
      " |  \n",
      " |  enable_vae_slicing(self)\n",
      " |      Enable sliced VAE decoding.\n",
      " |      \n",
      " |      When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n",
      " |      steps. This is useful to save some memory and allow larger batch sizes.\n",
      " |  \n",
      " |  enable_vae_tiling(self)\n",
      " |      Enable tiled VAE decoding.\n",
      " |      \n",
      " |      When this option is enabled, the VAE will split the input tensor into tiles to compute decoding and encoding in\n",
      " |      several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\n",
      " |  \n",
      " |  prepare_extra_step_kwargs(self, generator, eta)\n",
      " |  \n",
      " |  prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None)\n",
      " |  \n",
      " |  run_safety_checker(self, image, device, dtype)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Any)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  disable_attention_slicing(self)\n",
      " |      Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n",
      " |      back to computing attention in one step.\n",
      " |  \n",
      " |  disable_xformers_memory_efficient_attention(self)\n",
      " |      Disable memory efficient attention as implemented in xformers.\n",
      " |  \n",
      " |  enable_attention_slicing(self, slice_size: Union[str, int, NoneType] = 'auto')\n",
      " |      Enable sliced attention computation.\n",
      " |      \n",
      " |      When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n",
      " |      in several steps. This is useful to save some memory in exchange for a small speed decrease.\n",
      " |      \n",
      " |      Args:\n",
      " |          slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
      " |              When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
      " |              `\"max\"`, maximum amount of memory will be saved by running only one slice at a time. If a number is\n",
      " |              provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n",
      " |              must be a multiple of `slice_size`.\n",
      " |  \n",
      " |  enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None)\n",
      " |      Enable memory efficient attention as implemented in xformers.\n",
      " |      \n",
      " |      When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n",
      " |      time. Speed up at training time is not guaranteed.\n",
      " |      \n",
      " |      Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n",
      " |      is used.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          attention_op (`Callable`, *optional*):\n",
      " |              Override the default `None` operator for use as `op` argument to the\n",
      " |              [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n",
      " |              function of xFormers.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import torch\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
      " |      \n",
      " |      >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
      " |      >>> pipe = pipe.to(\"cuda\")\n",
      " |      >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
      " |      >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n",
      " |      >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
      " |      ```\n",
      " |  \n",
      " |  progress_bar(self, iterable=None, total=None)\n",
      " |  \n",
      " |  register_modules(self, **kwargs)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = False, variant: Optional[str] = None)\n",
      " |      Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n",
      " |      a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n",
      " |      method. The pipeline can easily be re-loaded using the [`~DiffusionPipeline.from_pretrained`] class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to which to save. Will be created if it doesn't exist.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified, weights are saved in the format pytorch_model.<variant>.bin.\n",
      " |  \n",
      " |  set_attention_slice(self, slice_size: Optional[int])\n",
      " |  \n",
      " |  set_progress_bar_config(self, **kwargs)\n",
      " |  \n",
      " |  set_use_memory_efficient_attention_xformers(self, valid: bool, attention_op: Optional[Callable] = None) -> None\n",
      " |  \n",
      " |  to(self, torch_device: Union[str, torch.device, NoneType] = None, torch_dtype: Optional[torch.dtype] = None, silence_dtype_warnings: bool = False)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  download(pretrained_model_name, **kwargs) -> Union[str, os.PathLike] from builtins.type\n",
      " |      Download and cache a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name (`str` or `os.PathLike`, *optional*):\n",
      " |              A string, the repository id (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |              hosted on the Hub.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the repository id (for example `CompVis/ldm-text2im-large-256`) of a pretrained\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called `pipeline.py` that defines\n",
      " |                    the custom pipeline.\n",
      " |      \n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current `main` branch of GitHub.\n",
      " |      \n",
      " |                  - A path to a *directory* (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              ðŸ§ª This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, take a look at [How to contribute a\n",
      " |              community pipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline).\n",
      " |      \n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to False, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to True, the model\n",
      " |              wonâ€™t be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"` when loading from the Hub and to local version of\n",
      " |          `diffusers` when loading from GitHub):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a diffusers version when loading a\n",
      " |              custom pipeline from GitHub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `os.PathLike`:\n",
      " |              A path to the downloaded pipeline.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], **kwargs) from builtins.type\n",
      " |      Instantiate a PyTorch diffusion pipeline from pre-trained pipeline weights.\n",
      " |      \n",
      " |      The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
      " |      \n",
      " |      The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
      " |      pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      " |      task.\n",
      " |      \n",
      " |      The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
      " |      weights are discarded.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* of a pretrained pipeline hosted inside a model repo on\n",
      " |                    https://huggingface.co/ Valid repo ids have to be located under a user or organization name, like\n",
      " |                    `CompVis/ldm-text2im-large-256`.\n",
      " |                  - A path to a *directory* containing pipeline weights saved using\n",
      " |                    [`~DiffusionPipeline.save_pretrained`], e.g., `./my_pipeline_directory/`.\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n",
      " |              will be automatically derived from the model's weights.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |                  This is an experimental feature and is likely to change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* of a custom pipeline hosted inside a model repo on\n",
      " |                    https://huggingface.co/. Valid repo ids have to be located under a user or organization name,\n",
      " |                    like `hf-internal-testing/diffusers-dummy-pipeline`.\n",
      " |      \n",
      " |                      <Tip>\n",
      " |      \n",
      " |                       It is required that the model repo has a file, called `pipeline.py` that defines the custom\n",
      " |                       pipeline.\n",
      " |      \n",
      " |                      </Tip>\n",
      " |      \n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    https://github.com/huggingface/diffusers/tree/main/examples/community. Valid file names have to\n",
      " |                    match exactly the file name without `.py` located under the above link, *e.g.*\n",
      " |                    `clip_guided_stable_diffusion`.\n",
      " |      \n",
      " |                      <Tip>\n",
      " |      \n",
      " |                       Community pipelines are always loaded from the current `main` branch of GitHub.\n",
      " |      \n",
      " |                      </Tip>\n",
      " |      \n",
      " |                  - A path to a *directory* containing a custom pipeline, e.g., `./my_pipeline_directory/`.\n",
      " |      \n",
      " |                      <Tip>\n",
      " |      \n",
      " |                       It is required that the directory has a file, called `pipeline.py` that defines the custom\n",
      " |                       pipeline.\n",
      " |      \n",
      " |                      </Tip>\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
      " |              Adding Custom\n",
      " |              Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n",
      " |      \n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"` when loading from the Hub and to local version of `diffusers` when loading from GitHub):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a diffusers version when loading a\n",
      " |              custom pipeline from GitHub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      " |              Please refer to the mirror site for more information. specify the folder name here.\n",
      " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
      " |              A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
      " |              parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
      " |              same device.\n",
      " |      \n",
      " |              To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
      " |              more information about each option see [designing a device\n",
      " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      " |          max_memory (`Dict`, *optional*):\n",
      " |              A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
      " |              GPU and the available CPU RAM if unset.\n",
      " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
      " |              If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
      " |          offload_state_dict (`bool`, *optional*):\n",
      " |              If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n",
      " |              RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n",
      " |              `True` when there is some disk offload.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading by not initializing the weights and only loading the pre-trained weights. This\n",
      " |              also tries to not use more than 1x model size in CPU memory (including peak memory) while loading the\n",
      " |              model. This is only supported when torch version >= 1.9.0. If you are using an older version of torch,\n",
      " |              setting this argument to `True` will raise an error.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the pipeline will load the `safetensors` weights if they're available **and** if the\n",
      " |              `safetensors` library is installed. If set to `True`, the pipeline will forcibly load the models from\n",
      " |              `safetensors` weights. If set to `False` the pipeline will *not* use `safetensors`.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to overwrite load - and saveable variables - *i.e.* the pipeline components - of the\n",
      " |              specific pipeline class. The overwritten components are then directly passed to the pipelines\n",
      " |              `__init__` method. See example below for more information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\n",
      " |              ignored when using `from_flax`.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |       It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n",
      " |       models](https://huggingface.co/docs/hub/models-gated#gated-models), *e.g.* `\"runwayml/stable-diffusion-v1-5\"`\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Activate the special [\"offline-mode\"](https://huggingface.co/diffusers/installation.html#offline-mode) to use\n",
      " |      this method in a firewalled environment.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      \n",
      " |      >>> # Download pipeline from huggingface.co and cache.\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
      " |      \n",
      " |      >>> # Download pipeline that requires an authorization token\n",
      " |      >>> # For more information on access tokens, please refer to this section\n",
      " |      >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      \n",
      " |      >>> # Use a different scheduler\n",
      " |      >>> from diffusers import LMSDiscreteScheduler\n",
      " |      \n",
      " |      >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
      " |      >>> pipeline.scheduler = scheduler\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  numpy_to_pil(images)\n",
      " |      Convert a numpy image or a batch of images to a PIL image.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  components\n",
      " |      The `self.components` property can be useful to run different pipelines with the same weights and\n",
      " |      configurations to not have to re-allocate memory.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import (\n",
      " |      ...     StableDiffusionPipeline,\n",
      " |      ...     StableDiffusionImg2ImgPipeline,\n",
      " |      ...     StableDiffusionInpaintPipeline,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> text2img = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      >>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n",
      " |      >>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary containing all the modules needed to initialize the pipeline.\n",
      " |  \n",
      " |  device\n",
      " |      Returns:\n",
      " |          `torch.device`: The torch device on which the pipeline is located.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  config_name = 'model_index.json'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      The only reason we overwrite `getattr` here is to gracefully deprecate accessing\n",
      " |      config attributes directly. See https://github.com/huggingface/diffusers/pull/3129\n",
      " |      \n",
      " |      Tihs funtion is mostly copied from PyTorch's __getattr__ overwrite:\n",
      " |      https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  register_to_config(self, **kwargs)\n",
      " |  \n",
      " |  save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      " |      Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n",
      " |      [`~ConfigMixin.from_config`] class method.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory where the configuration JSON file will be saved (will be created if it does not exist).\n",
      " |  \n",
      " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      " |      Save this instance to a JSON file.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file_path (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file in which this configuration instance's parameters will be saved.\n",
      " |  \n",
      " |  to_json_string(self) -> str\n",
      " |      Serializes this instance to a JSON string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: String containing all the attributes that make up this configuration instance in JSON format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  extract_init_dict(config_dict, **kwargs) from builtins.type\n",
      " |  \n",
      " |  from_config(config: Union[diffusers.configuration_utils.FrozenDict, Dict[str, Any]] = None, return_unused_kwargs=False, **kwargs) from builtins.type\n",
      " |      Instantiate a Python class from a config dictionary.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          config (`Dict[str, Any]`):\n",
      " |              A config dictionary from which the Python class will be instantiated. Make sure to only load\n",
      " |              configuration files of compatible classes.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether kwargs that are not consumed by the Python class should be returned or not.\n",
      " |      \n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to update the configuration object (after it is loaded) and initiate the Python class.\n",
      " |              `**kwargs` are directly passed to the underlying scheduler/model's `__init__` method and eventually\n",
      " |              overwrite same named arguments in `config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`ModelMixin`] or [`SchedulerMixin`]:\n",
      " |              A model or scheduler object instantiated from a config dictionary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler\n",
      " |      \n",
      " |      >>> # Download scheduler from huggingface.co and cache.\n",
      " |      >>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cifar10-32\")\n",
      " |      \n",
      " |      >>> # Instantiate DDIM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = DDIMScheduler.from_config(scheduler.config)\n",
      " |      \n",
      " |      >>> # Instantiate PNDM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = PNDMScheduler.from_config(scheduler.config)\n",
      " |      ```\n",
      " |  \n",
      " |  get_config_dict(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  load_config(pretrained_model_name_or_path: Union[str, os.PathLike], return_unused_kwargs=False, return_commit_hash=False, **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]] from builtins.type\n",
      " |      Load a model or scheduler configuration.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing model weights saved with\n",
      " |                    [`~ConfigMixin.save_config`].\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to False, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to True, the model\n",
      " |              wonâ€™t be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False):\n",
      " |              Whether unused keyword arguments of the config are returned.\n",
      " |          return_commit_hash (`bool`, *optional*, defaults to `False):\n",
      " |              Whether the `commit_hash` of the loaded configuration are returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `dict`:\n",
      " |              A dictionary of all the parameters stored in a JSON configuration file.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with\n",
      " |      `huggingface-cli login`. You can also activate the special\n",
      " |      [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to use this method in a\n",
      " |      firewalled environment.\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  config\n",
      " |      Returns the config of the class as a frozen dictionary\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, Any]`: Config of the class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  has_compatibles = False\n",
      " |  \n",
      " |  ignore_for_config = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.TextualInversionLoaderMixin:\n",
      " |  \n",
      " |  load_textual_inversion(self, pretrained_model_name_or_path: Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]], token: Union[str, List[str], NoneType] = None, **kwargs)\n",
      " |      Load textual inversion embeddings into the text encoder of stable diffusion pipelines. Both `diffusers` and\n",
      " |      `Automatic1111` formats are supported (see example below).\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This function is experimental and might change in the future.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike` or `List[str or os.PathLike]` or `Dict` or `List[Dict]`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      " |                    Valid model ids should have an organization name, like\n",
      " |                    `\"sd-concepts-library/low-poly-hd-logos-icons\"`.\n",
      " |                  - A path to a *directory* containing textual inversion weights, e.g.\n",
      " |                    `./my_text_inversion_directory/`.\n",
      " |                  - A path to a *file* containing textual inversion weights, e.g. `./my_text_inversions.pt`.\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |              Or a list of those elements.\n",
      " |          token (`str` or `List[str]`, *optional*):\n",
      " |              Override the token to use for the textual inversion weights. If `pretrained_model_name_or_path` is a\n",
      " |              list, then `token` must also be a list of equal length.\n",
      " |          weight_name (`str`, *optional*):\n",
      " |              Name of a custom weight file. This should be used in two cases:\n",
      " |      \n",
      " |                  - The saved textual inversion file is in `diffusers` format, but was saved under a specific weight\n",
      " |                    name, such as `text_inv.bin`.\n",
      " |                  - The saved textual inversion file is in the \"Automatic1111\" form.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `diffusers-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo (either remote in\n",
      " |              huggingface.co or downloaded locally), you can specify the folder name here.\n",
      " |      \n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      " |              Please refer to the mirror site for more information.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |       It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n",
      " |       models](https://huggingface.co/docs/hub/models-gated#gated-models).\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in `diffusers` format:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n",
      " |      \n",
      " |      prompt = \"A <cat-toy> backpack\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"cat-backpack.png\")\n",
      " |      ```\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in Automatic1111 format, make sure to first download the vector,\n",
      " |      e.g. from [civitAI](https://civitai.com/models/3036?modelVersionId=9857) and then load the vector locally:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n",
      " |      \n",
      " |      prompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"character.png\")\n",
      " |      ```\n",
      " |  \n",
      " |  maybe_convert_prompt(self, prompt: Union[str, List[str]], tokenizer: 'PreTrainedTokenizer')\n",
      " |      Maybe convert a prompt into a \"multi vector\"-compatible prompt. If the prompt includes a token that corresponds\n",
      " |      to a multi-vector textual inversion embedding, this function will process the prompt so that the special token\n",
      " |      is replaced with multiple special tokens each corresponding to one of the vectors. If the prompt has no textual\n",
      " |      inversion token or a textual inversion token that is a single vector, the input prompt is simply returned.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prompt (`str` or list of `str`):\n",
      " |              The prompt or prompts to guide the image generation.\n",
      " |          tokenizer (`PreTrainedTokenizer`):\n",
      " |              The tokenizer responsible for encoding the prompt into input tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or list of `str`: The converted prompt\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  load_lora_weights(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs)\n",
      " |      Load pretrained attention processor layers (such as LoRA) into [`UNet2DConditionModel`] and\n",
      " |      [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)).\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      We support loading A1111 formatted LoRA checkpoints in a limited capacity.\n",
      " |      \n",
      " |      This function is experimental and might change in the future.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      " |                    Valid model ids should have an organization name, like `google/ddpm-celebahq-256`.\n",
      " |                  - A path to a *directory* containing model weights saved using [`~ModelMixin.save_config`], e.g.,\n",
      " |                    `./my_model_directory/`.\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `diffusers-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo (either remote in\n",
      " |              huggingface.co or downloaded locally), you can specify the folder name here.\n",
      " |      \n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      " |              Please refer to the mirror site for more information.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n",
      " |      models](https://huggingface.co/docs/hub/models-gated#gated-models).\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  save_lora_weights(save_directory: Union[str, os.PathLike], unet_lora_layers: Dict[str, Union[torch.nn.modules.module.Module, torch.Tensor]] = None, text_encoder_lora_layers: Dict[str, torch.nn.modules.module.Module] = None, is_main_process: bool = True, weight_name: str = None, save_function: Callable = None, safe_serialization: bool = False) from builtins.type\n",
      " |      Save the LoRA parameters corresponding to the UNet and the text encoder.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to which to save. Will be created if it doesn't exist.\n",
      " |          unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the UNet. Specifying this helps to make the\n",
      " |              serialization process easier and cleaner. Values can be both LoRA torch.nn.Modules layers or torch\n",
      " |              weights.\n",
      " |          text_encoder_lora_layers (`Dict[str, torch.nn.Module] or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the `text_encoder`. Since the `text_encoder` comes from\n",
      " |              `transformers`, we cannot rejig it. That is why we have to explicitly pass the text encoder LoRA state\n",
      " |              dict. Values can be both LoRA torch.nn.Modules layers or torch weights.\n",
      " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the process calling this is the main process or not. Useful when in distributed training like\n",
      " |              TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n",
      " |              the main process to avoid race conditions.\n",
      " |          save_function (`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
      " |              need to replace `torch.save` by another method. Can be configured with the environment variable\n",
      " |              `DIFFUSERS_SAVE_MODE`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  lora_scale\n",
      " |  \n",
      " |  text_encoder_lora_attn_procs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  text_encoder_name = 'text_encoder'\n",
      " |  \n",
      " |  unet_name = 'unet'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.loaders.FromCkptMixin:\n",
      " |  \n",
      " |  from_ckpt(pretrained_model_link_or_path, **kwargs) from builtins.type\n",
      " |      Instantiate a PyTorch diffusion pipeline from pre-trained pipeline weights saved in the original .ckpt format.\n",
      " |      \n",
      " |      The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_link_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |                  - A link to the .ckpt file on the Hub. Should be in the format\n",
      " |                    `\"https://huggingface.co/<repo_id>/blob/main/<path_to_file>\"`\n",
      " |                  - A path to a *file* containing all pipeline weights.\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n",
      " |              will be automatically derived from the model's weights.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the pipeline will load the `safetensors` weights if they're available **and** if the\n",
      " |              `safetensors` library is installed. If set to `True`, the pipeline will forcibly load the models from\n",
      " |              `safetensors` weights. If set to `False` the pipeline will *not* use `safetensors`.\n",
      " |          extract_ema (`bool`, *optional*, defaults to `False`): Only relevant for\n",
      " |              checkpoints that have both EMA and non-EMA weights. Whether to extract the EMA weights or not. Defaults\n",
      " |              to `False`. Pass `True` to extract the EMA weights. EMA weights usually yield higher quality images for\n",
      " |              inference. Non-EMA weights are usually better to continue fine-tuning.\n",
      " |          upcast_attention (`bool`, *optional*, defaults to `None`):\n",
      " |              Whether the attention computation should always be upcasted. This is necessary when running stable\n",
      " |          image_size (`int`, *optional*, defaults to 512):\n",
      " |              The image size that the model was trained on. Use 512 for Stable Diffusion v1.X and Stable Diffusion v2\n",
      " |              Base. Use 768 for Stable Diffusion v2.\n",
      " |          prediction_type (`str`, *optional*):\n",
      " |              The prediction type that the model was trained on. Use `'epsilon'` for Stable Diffusion v1.X and Stable\n",
      " |              Diffusion v2 Base. Use `'v_prediction'` for Stable Diffusion v2.\n",
      " |          num_in_channels (`int`, *optional*, defaults to None):\n",
      " |              The number of input channels. If `None`, it will be automatically inferred.\n",
      " |          scheduler_type (`str`, *optional*, defaults to 'pndm'):\n",
      " |              Type of scheduler to use. Should be one of `[\"pndm\", \"lms\", \"heun\", \"euler\", \"euler-ancestral\", \"dpm\",\n",
      " |              \"ddim\"]`.\n",
      " |          load_safety_checker (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to load the safety checker or not. Defaults to `True`.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to overwrite load - and saveable variables - *i.e.* the pipeline components - of the\n",
      " |              specific pipeline class. The overwritten components are then directly passed to the pipelines\n",
      " |              `__init__` method. See example below for more information.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import StableDiffusionPipeline\n",
      " |      \n",
      " |      >>> # Download pipeline from huggingface.co and cache.\n",
      " |      >>> pipeline = StableDiffusionPipeline.from_ckpt(\n",
      " |      ...     \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # Download pipeline from local file\n",
      " |      >>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt\n",
      " |      >>> pipeline = StableDiffusionPipeline.from_ckpt(\"./v1-5-pruned-emaonly\")\n",
      " |      \n",
      " |      >>> # Enable float16 and move to GPU\n",
      " |      >>> pipeline = StableDiffusionPipeline.from_ckpt(\n",
      " |      ...     \"https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt\",\n",
      " |      ...     torch_dtype=torch.float16,\n",
      " |      ... )\n",
      " |      >>> pipeline.to(\"cuda\")\n",
      " |      ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [2 ** x for x in range(0, 8)]\n",
    "steps = 50\n",
    "cfg_scale = 15\n",
    "prompt = \"postapocalyptic steampunk city, exploration, cinematic, realistic, hyper detailed, photorealistic maximum detail, volumetric light, (((focus))), wide-angle, (((brightly lit))), (((vegetation))), lightning, vines, destruction, devastation, wartorn, ruins\"\n",
    "# prompt = \"detailed portrait beautiful Neon Operator Girl, cyberpunk futuristic neon, reflective puffy coat, decorated with traditional Japanese ornaments by Ismail inceoglu dragan bibin hans thoma greg rutkowski Alexandros Pyromallis Nekro Rene Maritte Illustrated, Perfect face, fine details, realistic shaded, fine-face, pretty face\"\n",
    "negative_prompt = \"(((blurry))), ((foggy)), (((dark))), ((monochrome)), sun, (((depth of field)))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f012d0f783414bfcba2b0231a939ae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 4, it/s: 18.83, time: 10.62\n"
     ]
    }
   ],
   "source": [
    "# prewarm\n",
    "batch_size = 4\n",
    "t0 = time.time()\n",
    "_ = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    num_images_per_prompt=batch_size,\n",
    "    guidance_scale=cfg_scale,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    ).images\n",
    "t1 = time.time()\n",
    "its = steps * batch_size / (t1 - t0)\n",
    "print(\"batch_size {}, it/s: {}, time: {}\".format(batch_size, round(its, 2), round((t1 - t0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [1, 1, 2, 2, 4, 4, 8, 8, 16, 16, 32, 32, 64, 64, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b51794d6a9547c19fc63b095ce18472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1, it/s: 15.61, time: 3.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6b0381baae4090b9fae1d9eee8028e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1, it/s: 15.63, time: 3.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565778290c814b28ad88596bbe4cc8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2, it/s: 0.78, time: 128.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814f0a34306e4b09a301272b77560df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2, it/s: 17.4, time: 5.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9f971b1585448f96e7ddd9d41ce907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 4, it/s: 17.65, time: 11.33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95376d953b7a4feebf787b1efaf7391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 4, it/s: 17.61, time: 11.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd78f371601407a8810a53609ac7b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 8, it/s: 3.97, time: 100.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e786657a2b2c48208ea10f2255473a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 8, it/s: 17.26, time: 23.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4475bd80c4394ef4b6c2b64e05fc29e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 16, it/s: 6.98, time: 114.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05f02d70f6d40eb8383405dd7499c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 16, it/s: 17.18, time: 46.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f6eee943ed45d09d3629f2c48520e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32, it/s: 10.69, time: 149.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64aa00d6f82e445188f37d7fa014dbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32, it/s: 15.53, time: 103.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e44e1e45e59417ea9d1978d53c5d0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 64, OOM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de787c7784994519a48d610377b61549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 64, OOM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69ee47de8924bd2acabc62ce64f7bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 128, OOM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2a1e2b40174f2f8b9fc14e1b13aac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 128, OOM\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for batch_size in batch_size_list:\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        images = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=steps,\n",
    "            num_images_per_prompt=batch_size,\n",
    "            guidance_scale=cfg_scale,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            ).images\n",
    "        t1 = time.time()\n",
    "        its = steps * batch_size / (t1 - t0)\n",
    "        print(\"batch_size {}, it/s: {}, time: {}\".format(batch_size, round(its, 2), round((t1 - t0), 2)))\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        print(\"batch_size {}, OOM\".format(batch_size))\n",
    "        its = 0\n",
    "    except Exception:\n",
    "        print(traceback.print_exc())\n",
    "    result.append(round(its, 2))\n",
    "result_jit = result[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.63, 17.4, 17.61, 17.26, 17.18, 15.53, 0, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
